{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f95231cd",
   "metadata": {},
   "source": [
    "### Cora dataset ideas:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56edd28d",
   "metadata": {},
   "source": [
    "\n",
    "- Feature type: Bag-of-Words (binary)\n",
    "\n",
    "- Each node has a 1433-dimensional feature vector\n",
    "\n",
    "- Each dimension corresponds to a unique word in the vocabulary\n",
    "\n",
    "- Values are binary:\n",
    "\n",
    "- 1 : the word appers in the paper\n",
    "\n",
    "- 0 : the word does not appear\n",
    "\n",
    "### 1. Node Classification\n",
    "   - Thousands of papers are published every week —> manual categorization doesn’t scale.\n",
    "\n",
    "  #### plan:\n",
    "  - apply Shallow Embeddings with Node2Vec (only uses graph structure 'no features') to show classifiction performance only utilizing citations links\n",
    "  - apply Deep Embeddings (GNNs) like GCN and GAT (makes use of node features) and perform classifiction\n",
    "  - analysis (compare methods accuracies , t-SNE plots of embeddings colored by class (for each method),Discuss trade-offs (scalability, expressiveness, transductive vs. inductive))\n",
    "\n",
    "\n",
    "\n",
    "### 2. Link Prediction\n",
    "  - Authors often miss relevant prior work.\n",
    "  - Citation recommendation during writing\n",
    "  - Related-work discovery\n",
    "  - Recommending papers that an author should read or cite based on the graph structure and/or content similarity.\n",
    "\n",
    "  #### plan:\n",
    "  - edge splitting and generating negative samples \n",
    "  - learn embeddings with link prediction objective (not sure if node2vec can use same embeddings generated for node classification or not, but for gnn we need new embeddings cause loss is different)\n",
    "  - check scoring methods (For each pair (u, v) in the test set, compute a score using the embeddings using Dot product/Cosine similarity/mlp classifier )\n",
    "  - evaluate predictions \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c36358d",
   "metadata": {},
   "source": [
    "### Twitch dataset ideas:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07807ba1",
   "metadata": {},
   "source": [
    "There are 5 languages, English, French, German, Italian, and Spanish. Each language community is disconnected from the others. Each node has the following features:\n",
    "id,days,mature,views,partner,new_id\n",
    "### 1. Node Classification\n",
    "   - Automatically flagging streamers who use explicit language —> manual review doesn't scale across 32K+ streamers and multiple languages\n",
    "\n",
    "  #### plan:\n",
    "  - Approach 1: Per-language models (baseline):\n",
    "    -   train and evaluate within each language separately:\n",
    "    - For each language graph:\n",
    "        - Node2Vec → logistic regression (structure only, no features)\n",
    "        - GCN / GAT (structure + features, end-to-end)\n",
    "        - Stratified train/val/test split within that language\n",
    "    - Compare: does graph structure actually help? Are some languages easier to classify than others? Does graph density (DE is dense, ENGB is sparse) affect GNN performance?\n",
    "- Approach 2: Transfer learning across languages\n",
    "    - All languages share the same feature space:\n",
    "    - Train a GCN/GAT on one language → test on a different language (zero-shot transfer)\n",
    "    - So, we train on DE (largest, densest) → test on RU (smallest)\n",
    "    - This tests whether the relationship between gaming preferences, social connections, and explicit language use is language/culture-invariant\n",
    "    Compare transfer accuracy across all language pairs,  which transfers well, which doesn't?\n",
    "    - create a Transfer Matrix: \n",
    "        - Diagonal = train and test on the same language (normal in-domain performance, the upper bound)\n",
    "        - Off-diagonal = transfer performance (train on row, test on column)\n",
    "        - For each cell in the matrix: Accuracy, F1-score, AUC-ROC (since it's binary classification and mature may be imbalanced)\n",
    "    - we analyze: \n",
    "        - How much does transfer performance drop vs. in-domain? If off-diagonal scores are close to diagonal → explicit language behavior follows universal social patterns across cultures. If there's a big drop → the relationship between features/structure and mature is culture-specific\n",
    "        - Which language transfers best as a source?\n",
    "        - Are some language pairs more similar? use a heatmap\n",
    "    - Per-language accuracy/F1/AUC-ROC comparison table\n",
    "t-SNE of embeddings colored by (a) mature and (b) language — do language communities cluster separately even without explicit language features?\n",
    "Feature-only baseline (no graph, just logistic regression on features) — how much does the graph add?\n",
    "Transfer learning matrix: train on row language, test on column language\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 2. Link Prediction\n",
    "  - which streamers would be friends based on their gaming preferences and existing social ties\n",
    "  - Since there are no cross-language edges, link prediction only makes sense within each language. We should:\n",
    "- Split edges per language (e.g., 85/5/10 train/val/test)\n",
    "- Generate negative samples only from same-language node pairs (sampling random pairs across the full graph would be trivially easy since cross-language edges don't exist)\n",
    "- Ensure the training graph stays connected within each component after edge removal\n",
    "\n",
    "#### plan:\n",
    "  - Node2Vec: Embed on the training subgraph per language, score test edges via dot product / cosine similarity\n",
    "- GCN/GAT with link prediction head: Train with binary cross-entropy on edge existence — separate from node classification since the loss and training signal are fundamentally different\n",
    "- Scoring: For each candidate pair (u, v), try dot product, cosine similarity, and Hadamard product → MLP\n",
    "- Evaluation\n",
    "    - AUC-ROC and Average Precision per language\n",
    "    - Compare: do denser networks (DE, FR) have better link prediction than sparser ones (ENGB, RU)? Denser graphs have more training signal but also more possible negatives.\n",
    "    - Does including node features improve link prediction over structure-only methods? (i.e., are streamers friends because they like the same games, or for other reasons?)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
