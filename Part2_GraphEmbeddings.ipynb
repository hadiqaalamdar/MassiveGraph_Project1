{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f95231cd",
   "metadata": {},
   "source": [
    "### Cora dataset ideas:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56edd28d",
   "metadata": {},
   "source": [
    "\n",
    "- Feature type: Bag-of-Words (binary)\n",
    "\n",
    "- Each node has a 1433-dimensional feature vector\n",
    "\n",
    "- Each dimension corresponds to a unique word in the vocabulary\n",
    "\n",
    "- Values are binary:\n",
    "\n",
    "- 1 : the word appers in the paper\n",
    "\n",
    "- 0 : the word does not appear\n",
    "\n",
    "### 1. Node Classification\n",
    "   - Thousands of papers are published every week —> manual categorization doesn't scale.\n",
    "\n",
    "  #### plan:\n",
    "  - apply Shallow Embeddings with Node2Vec (only uses graph structure 'no features') to show classifiction performance only utilizing citations links\n",
    "  - apply Deep Embeddings (GNNs) like GCN and GAT (makes use of node features) and perform classifiction\n",
    "  - analysis (compare methods accuracies , t-SNE plots of embeddings colored by class (for each method),Discuss trade-offs (scalability, expressiveness, transductive vs. inductive))\n",
    "\n",
    "\n",
    "\n",
    "### 2. Link Prediction\n",
    "  - Authors often miss relevant prior work.\n",
    "  - Citation recommendation during writing\n",
    "  - Related-work discovery\n",
    "  - Recommending papers that an author should read or cite based on the graph structure and/or content similarity.\n",
    "\n",
    "  #### plan:\n",
    "  - edge splitting and generating negative samples \n",
    "  - learn embeddings with link prediction objective (not sure if node2vec can use same embeddings generated for node classification or not, but for gnn we need new embeddings cause loss is different)\n",
    "  - check scoring methods (For each pair (u, v) in the test set, compute a score using the embeddings using Dot product/Cosine similarity/mlp classifier )\n",
    "  - evaluate predictions \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c36358d",
   "metadata": {},
   "source": [
    "### Twitch dataset ideas:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07807ba1",
   "metadata": {},
   "source": [
    "There are 5 languages, English, French, German, Italian, and Spanish. Each language community is disconnected from the others. Each node has the following features:\n",
    "id,days,mature,views,partner,new_id\n",
    "### 1. Node Classification\n",
    "   - Automatically flagging streamers who use explicit language —> manual review doesn't scale across 32K+ streamers and multiple languages\n",
    "\n",
    "  #### plan:\n",
    "  - Approach 1: Per-language models (baseline):\n",
    "    -   train and evaluate within each language separately:\n",
    "    - For each language graph:\n",
    "        - Node2Vec → logistic regression (structure only, no features)\n",
    "        - GCN / GAT (structure + features, end-to-end)\n",
    "        - Stratified train/val/test split within that language\n",
    "    - Compare: does graph structure actually help? Are some languages easier to classify than others? Does graph density (DE is dense, ENGB is sparse) affect GNN performance?\n",
    "- Approach 2: Transfer learning across languages\n",
    "    - All languages share the same feature space:\n",
    "    - Train a GCN/GAT on one language → test on a different language (zero-shot transfer)\n",
    "    - So, we train on DE (largest, densest) → test on RU (smallest)\n",
    "    - This tests whether the relationship between gaming preferences, social connections, and explicit language use is language/culture-invariant\n",
    "    Compare transfer accuracy across all language pairs,  which transfers well, which doesn't?\n",
    "    - create a Transfer Matrix: \n",
    "        - Diagonal = train and test on the same language (normal in-domain performance, the upper bound)\n",
    "        - Off-diagonal = transfer performance (train on row, test on column)\n",
    "        - For each cell in the matrix: Accuracy, F1-score, AUC-ROC (since it's binary classification and mature may be imbalanced)\n",
    "    - we analyze: \n",
    "        - How much does transfer performance drop vs. in-domain? If off-diagonal scores are close to diagonal → explicit language behavior follows universal social patterns across cultures. If there's a big drop → the relationship between features/structure and mature is culture-specific\n",
    "        - Which language transfers best as a source?\n",
    "        - Are some language pairs more similar? use a heatmap\n",
    "    - Per-language accuracy/F1/AUC-ROC comparison table\n",
    "t-SNE of embeddings colored by (a) mature and (b) language — do language communities cluster separately even without explicit language features?\n",
    "Feature-only baseline (no graph, just logistic regression on features) — how much does the graph add?\n",
    "Transfer learning matrix: train on row language, test on column language\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 2. Link Prediction\n",
    "  - which streamers would be friends based on their gaming preferences and existing social ties\n",
    "  - Since there are no cross-language edges, link prediction only makes sense within each language. We should:\n",
    "- Split edges per language (e.g., 85/5/10 train/val/test)\n",
    "- Generate negative samples only from same-language node pairs (sampling random pairs across the full graph would be trivially easy since cross-language edges don't exist)\n",
    "- Ensure the training graph stays connected within each component after edge removal\n",
    "\n",
    "#### plan:\n",
    "  - Node2Vec: Embed on the training subgraph per language, score test edges via dot product / cosine similarity\n",
    "- GCN/GAT with link prediction head: Train with binary cross-entropy on edge existence — separate from node classification since the loss and training signal are fundamentally different\n",
    "- Scoring: For each candidate pair (u, v), try dot product, cosine similarity, and Hadamard product → MLP\n",
    "- Evaluation\n",
    "    - AUC-ROC and Average Precision per language\n",
    "    - Compare: do denser networks (DE, FR) have better link prediction than sparser ones (ENGB, RU)? Denser graphs have more training signal but also more possible negatives.\n",
    "    - Does including node features improve link prediction over structure-only methods? (i.e., are streamers friends because they like the same games, or for other reasons?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Cora Node Classification with Shallow Embeddings (Node2Vec)\n",
    "\n",
    "**Goal:** Classify papers into 7 categories using **only the citation graph structure** (no BoW features).  \n",
    "This establishes a structure-only baseline before adding node features with GNNs.\n",
    "\n",
    "**Pipeline:**\n",
    "1. Load and parse the raw Cora files (`cora.content` and `cora.cites`)\n",
    "2. Build a PyTorch Geometric `Data` object\n",
    "3. Train Node2Vec to learn 128-d embeddings from random walks on the citation graph\n",
    "4. Train a Logistic Regression classifier on the learned embeddings\n",
    "5. Evaluate with accuracy, per-class F1, and confusion matrix\n",
    "6. Visualize embeddings with t-SNE colored by class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5",
   "metadata": {},
   "source": [
    "### 1.1 — Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3d4e5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Imports\n",
    "# =============================================================================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import Node2Vec     # Node2Vec implementation from PyG\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    ")\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Use GPU if available: CUDA (NVIDIA) → MPS (Apple Silicon) → CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6a7",
   "metadata": {},
   "source": [
    "### 1.2 — Load and Parse the Cora Dataset (from raw files)\n",
    "\n",
    "We load the data **manually** from the raw `.content` and `.cites` files  \n",
    "so we have full control and visibility into what's happening.\n",
    "\n",
    "- `cora.content`: each line is `<paper_id> <1433 binary features> <class_label>`\n",
    "- `cora.cites`: each line is `<cited_paper_id> <citing_paper_id>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5f6a7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers (nodes): 2708\n",
      "Feature matrix shape:     (2708, 1433)\n",
      "Unique classes:           ['Case_Based', 'Genetic_Algorithms', 'Neural_Networks', 'Probabilistic_Methods', 'Reinforcement_Learning', 'Rule_Learning', 'Theory']\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 1.2a  Load cora.content — node features and labels\n",
    "# =============================================================================\n",
    "DATA_DIR = \"cora\"  # relative path to the cora folder\n",
    "\n",
    "# Read the content file: <paper_id> <1433 binary features> <class_label>\n",
    "content_path = os.path.join(DATA_DIR, \"cora.content\")\n",
    "content_data = []\n",
    "with open(content_path, 'r') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split('\\t')\n",
    "        content_data.append(parts)\n",
    "\n",
    "# Parse into structured arrays\n",
    "paper_ids   = [row[0] for row in content_data]        # string IDs of each paper\n",
    "features    = np.array([row[1:-1] for row in content_data], dtype=np.float32)  # 1433-d BoW\n",
    "labels_str  = [row[-1] for row in content_data]       # class label strings\n",
    "\n",
    "print(f\"Number of papers (nodes): {len(paper_ids)}\")\n",
    "print(f\"Feature matrix shape:     {features.shape}\")\n",
    "print(f\"Unique classes:           {sorted(set(labels_str))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6a7b8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class mapping:\n",
      "  0: Case_Based                     (298 papers)\n",
      "  1: Genetic_Algorithms             (418 papers)\n",
      "  2: Neural_Networks                (818 papers)\n",
      "  3: Probabilistic_Methods          (426 papers)\n",
      "  4: Reinforcement_Learning         (217 papers)\n",
      "  5: Rule_Learning                  (180 papers)\n",
      "  6: Theory                         (351 papers)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 1.2b  Encode labels as integers and create a mapping\n",
    "# =============================================================================\n",
    "\n",
    "# Create a sorted list of unique class names for consistent ordering\n",
    "class_names = sorted(set(labels_str))\n",
    "class_to_idx = {name: idx for idx, name in enumerate(class_names)}\n",
    "\n",
    "# Convert string labels to integer labels\n",
    "labels = np.array([class_to_idx[l] for l in labels_str])\n",
    "\n",
    "print(\"Class mapping:\")\n",
    "for name, idx in class_to_idx.items():\n",
    "    count = (labels == idx).sum()\n",
    "    print(f\"  {idx}: {name:30s} ({count} papers)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7b8c9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges loaded: 5429\n",
      "Edges skipped (missing nodes): 0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 1.2c  Build a mapping from paper_id → contiguous node index (0..N-1)\n",
    "# =============================================================================\n",
    "# PyTorch Geometric expects nodes to be indexed 0, 1, ..., N-1\n",
    "paper_id_to_idx = {pid: i for i, pid in enumerate(paper_ids)}\n",
    "\n",
    "# =============================================================================\n",
    "# 1.2d  Load cora.cites — citation edges\n",
    "# =============================================================================\n",
    "cites_path = os.path.join(DATA_DIR, \"cora.cites\")\n",
    "edges = []\n",
    "skipped = 0\n",
    "\n",
    "with open(cites_path, 'r') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split('\\t')\n",
    "        cited  = parts[0]   # paper being cited\n",
    "        citing = parts[1]   # paper doing the citing\n",
    "        \n",
    "        # Some edges may reference papers not in the content file — skip them\n",
    "        if cited in paper_id_to_idx and citing in paper_id_to_idx:\n",
    "            src = paper_id_to_idx[citing]  # citing  → source\n",
    "            dst = paper_id_to_idx[cited]   # cited   → destination\n",
    "            edges.append([src, dst])\n",
    "        else:\n",
    "            skipped += 1\n",
    "\n",
    "edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()  # shape [2, num_edges]\n",
    "\n",
    "print(f\"Number of edges loaded: {edge_index.shape[1]}\")\n",
    "print(f\"Edges skipped (missing nodes): {skipped}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8c9d0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Graph summary:\n",
      "  Nodes:               2708\n",
      "  Edges (undirected):  10556\n",
      "  Node features dim:   1433\n",
      "  Classes:             7\n",
      "  Avg degree:          3.9\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 1.2e  Build the PyTorch Geometric Data object\n",
    "# =============================================================================\n",
    "# Even though Node2Vec won't use features, we store them in the Data object\n",
    "# so the same object can be reused later for GCN/GAT experiments.\n",
    "\n",
    "x = torch.tensor(features, dtype=torch.float)           # [2708, 1433]\n",
    "y = torch.tensor(labels, dtype=torch.long)               # [2708]\n",
    "\n",
    "# Make the graph undirected: if paper A cites paper B, we add edges A→B and B→A.\n",
    "# This is standard for citation networks because information flows both ways\n",
    "# (citing and being cited both indicate topical similarity).\n",
    "from torch_geometric.utils import to_undirected\n",
    "edge_index_undirected = to_undirected(edge_index)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index_undirected, y=y)\n",
    "data.num_classes = len(class_names)\n",
    "\n",
    "print(f\"\\nGraph summary:\")\n",
    "print(f\"  Nodes:               {data.num_nodes}\")\n",
    "print(f\"  Edges (undirected):  {data.num_edges}\")\n",
    "print(f\"  Node features dim:   {data.num_node_features}\")\n",
    "print(f\"  Classes:             {data.num_classes}\")\n",
    "print(f\"  Avg degree:          {data.num_edges / data.num_nodes:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2",
   "metadata": {},
   "source": [
    "### 1.3 — Train / Validation / Test Split\n",
    "\n",
    "We use a stratified random split to ensure each class is represented proportionally  \n",
    "in each subset. This is especially important because the class distribution is imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0e1f2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train nodes: 1624 (60%)\n",
      "Val   nodes: 542 (20%)\n",
      "Test  nodes: 542 (20%)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Create stratified train/val/test masks (60% / 20% / 20%)\n",
    "# =============================================================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "num_nodes = data.num_nodes\n",
    "indices = np.arange(num_nodes)\n",
    "\n",
    "# First split: 60% train, 40% temp (val + test)\n",
    "train_idx, temp_idx = train_test_split(\n",
    "    indices, test_size=0.4, stratify=labels, random_state=SEED\n",
    ")\n",
    "\n",
    "# Second split: 50% of temp → 20% val, 20% test\n",
    "val_idx, test_idx = train_test_split(\n",
    "    temp_idx, test_size=0.5, stratify=labels[temp_idx], random_state=SEED\n",
    ")\n",
    "\n",
    "# Create boolean masks for PyG compatibility\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "val_mask   = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask  = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "train_mask[train_idx] = True\n",
    "val_mask[val_idx]     = True\n",
    "test_mask[test_idx]   = True\n",
    "\n",
    "data.train_mask = train_mask\n",
    "data.val_mask   = val_mask\n",
    "data.test_mask  = test_mask\n",
    "\n",
    "print(f\"Train nodes: {train_mask.sum().item()} ({train_mask.sum().item()/num_nodes*100:.0f}%)\")\n",
    "print(f\"Val   nodes: {val_mask.sum().item()} ({val_mask.sum().item()/num_nodes*100:.0f}%)\")\n",
    "print(f\"Test  nodes: {test_mask.sum().item()} ({test_mask.sum().item()/num_nodes*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4",
   "metadata": {},
   "source": [
    "### 1.4 — Train Node2Vec Embeddings\n",
    "\n",
    "**Node2Vec** learns low-dimensional embeddings by performing biased random walks  \n",
    "on the graph and then applying a Skip-Gram model (similar to Word2Vec on text).\n",
    "\n",
    "Key parameters:\n",
    "- `embedding_dim`: dimensionality of the learned embeddings (128)\n",
    "- `walk_length`: number of steps in each random walk (80)\n",
    "- `context_size`: window size for the Skip-Gram objective (10)\n",
    "- `walks_per_node`: how many walks to start from each node (10)\n",
    "- `p` (return parameter): controls likelihood of revisiting the previous node  \n",
    "  - p < 1 → more likely to backtrack (BFS-like, captures local structure)  \n",
    "  - p > 1 → less likely to backtrack (DFS-like, captures global structure)\n",
    "- `q` (in-out parameter): controls whether the walk explores outward or stays local  \n",
    "  - q < 1 → biased toward exploring farther nodes (DFS-like)\n",
    "  - q > 1 → biased toward staying near the start (BFS-like)\n",
    "\n",
    "**Important:** Node2Vec uses **only the graph structure** (edges).  \n",
    "It does NOT use the 1433-d BoW features — that's intentional. We want to see  \n",
    "how much classification performance we can get from citations alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2a3b4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node2Vec model initialized:\n",
      "  Embedding matrix shape: (2708, 128)\n",
      "  Total trainable params: 346,624\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Node2Vec Model Configuration\n",
    "# =============================================================================\n",
    "# These hyperparameters follow the original Node2Vec paper (Grover & Leskovec, 2016)\n",
    "# with slight adjustments for the Cora graph size.\n",
    "\n",
    "EMBEDDING_DIM = 128    # Dimensionality of node embeddings\n",
    "WALK_LENGTH   = 80     # Steps per random walk\n",
    "CONTEXT_SIZE  = 10     # Skip-Gram window size\n",
    "WALKS_PER_NODE = 10    # Number of random walks starting from each node\n",
    "P = 1.0                # Return parameter (p=1, q=1 → DeepWalk-equivalent)\n",
    "Q = 1.0                # In-out parameter\n",
    "NUM_EPOCHS    = 100    # Training epochs for the Skip-Gram objective\n",
    "BATCH_SIZE    = 256    # Batch size for stochastic gradient descent\n",
    "LR            = 0.01   # Learning rate\n",
    "\n",
    "# Initialize the Node2Vec model from PyTorch Geometric\n",
    "# NOTE: This only takes edge_index — it does NOT use node features (x)\n",
    "node2vec_model = Node2Vec(\n",
    "    edge_index=data.edge_index,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    walk_length=WALK_LENGTH,\n",
    "    context_size=CONTEXT_SIZE,\n",
    "    walks_per_node=WALKS_PER_NODE,\n",
    "    p=P,\n",
    "    q=Q,\n",
    "    num_nodes=data.num_nodes,\n",
    "    sparse=True,           # Use sparse gradients for efficiency\n",
    ").to(device)\n",
    "\n",
    "print(f\"Node2Vec model initialized:\")\n",
    "print(f\"  Embedding matrix shape: ({data.num_nodes}, {EMBEDDING_DIM})\")\n",
    "print(f\"  Total trainable params: {sum(p.numel() for p in node2vec_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3b4c5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/100 — Loss: 7.6390\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# pos_rw: positive random walk pairs (nodes that co-occur in walks)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# neg_rw: negative samples (random node pairs, unlikely to be related)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m loss \u001b[38;5;241m=\u001b[39m node2vec_model\u001b[38;5;241m.\u001b[39mloss(pos_rw\u001b[38;5;241m.\u001b[39mto(device), neg_rw\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m---> 29\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     31\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    646\u001b[0m     )\n\u001b[0;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/autograd/graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Node2Vec Training Loop\n",
    "# =============================================================================\n",
    "# The training process:\n",
    "# 1. Generate random walks from the graph (biased by p, q)\n",
    "# 2. Treat walks as \"sentences\" and nodes as \"words\"\n",
    "# 3. Apply Skip-Gram with negative sampling to learn embeddings\n",
    "#    that place co-occurring nodes (in walks) close together\n",
    "\n",
    "# DataLoader samples random walks and creates Skip-Gram training pairs\n",
    "loader = node2vec_model.loader(batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "# Using SparseAdam optimizer — efficient for sparse embedding gradients\n",
    "optimizer = torch.optim.SparseAdam(node2vec_model.parameters(), lr=LR)\n",
    "\n",
    "# Training\n",
    "node2vec_model.train()\n",
    "losses = []  # track loss for visualization\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for pos_rw, neg_rw in loader:\n",
    "        optimizer.zero_grad()\n",
    "        # pos_rw: positive random walk pairs (nodes that co-occur in walks)\n",
    "        # neg_rw: negative samples (random node pairs, unlikely to be related)\n",
    "        loss = node2vec_model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    losses.append(avg_loss)\n",
    "    \n",
    "    # # Print progress every 10 epochs\n",
    "    # if epoch % 10 == 0 or epoch == 1:\n",
    "    print(f\"Epoch {epoch:3d}/{NUM_EPOCHS} — Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"\\nNode2Vec training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Plot Training Loss\n",
    "# =============================================================================\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(1, NUM_EPOCHS + 1), losses, color='#2196F3', linewidth=1.5)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (Skip-Gram + Negative Sampling)')\n",
    "plt.title('Node2Vec Training Loss on Cora')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6e7f8",
   "metadata": {},
   "source": [
    "### 1.5 — Extract Embeddings\n",
    "\n",
    "After training, each node has a learned 128-dimensional embedding vector.  \n",
    "These embeddings encode the **structural role and neighborhood** of each paper  \n",
    "in the citation graph — without ever seeing the paper's content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Extract the trained embeddings for all nodes\n",
    "# =============================================================================\n",
    "node2vec_model.eval()\n",
    "\n",
    "# Get embeddings: shape [num_nodes, embedding_dim] = [2708, 128]\n",
    "with torch.no_grad():\n",
    "    embeddings = node2vec_model().detach().cpu().numpy()\n",
    "\n",
    "print(f\"Embedding matrix shape: {embeddings.shape}\")\n",
    "print(f\"\\nSample embedding (node 0, first 10 dims):\")\n",
    "print(f\"  {embeddings[0, :10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8a9b0",
   "metadata": {},
   "source": [
    "### 1.6 — Node Classification with Logistic Regression\n",
    "\n",
    "Now we use the Node2Vec embeddings as input features to a **Logistic Regression** classifier.  \n",
    "This is the standard evaluation protocol for shallow graph embeddings:\n",
    "\n",
    "1. Learn embeddings (unsupervised, structure-only)\n",
    "2. Use them as fixed features for a simple downstream classifier\n",
    "\n",
    "The classifier never sees the original BoW features — only the 128-d structural embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a9b0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Prepare train / val / test features and labels from the embeddings\n",
    "# =============================================================================\n",
    "X_train = embeddings[train_mask.numpy()]\n",
    "y_train = labels[train_mask.numpy()]\n",
    "\n",
    "X_val = embeddings[val_mask.numpy()]\n",
    "y_val = labels[val_mask.numpy()]\n",
    "\n",
    "X_test = embeddings[test_mask.numpy()]\n",
    "y_test = labels[test_mask.numpy()]\n",
    "\n",
    "print(f\"Train: {X_train.shape[0]} samples\")\n",
    "print(f\"Val:   {X_val.shape[0]} samples\")\n",
    "print(f\"Test:  {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b0c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Train Logistic Regression on Node2Vec embeddings\n",
    "# =============================================================================\n",
    "# We use L2 regularization (default) and increase max_iter for convergence.\n",
    "# The classifier is simple by design — the embedding quality determines performance.\n",
    "\n",
    "clf = LogisticRegression(\n",
    "    max_iter=2000,\n",
    "    multi_class='multinomial',   # 7-class classification\n",
    "    solver='lbfgs',              # good default for multinomial\n",
    "    random_state=SEED,\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation and test sets\n",
    "y_val_pred  = clf.predict(X_val)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "val_acc  = accuracy_score(y_val, y_val_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"  Node2Vec + Logistic Regression Results\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"  Validation Accuracy: {val_acc:.4f} ({val_acc*100:.1f}%)\")\n",
    "print(f\"  Test Accuracy:       {test_acc:.4f} ({test_acc*100:.1f}%)\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c1d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Detailed Classification Report (per-class precision, recall, F1)\n",
    "# =============================================================================\n",
    "print(\"\\nDetailed Classification Report (Test Set):\")\n",
    "print(\"=\" * 65)\n",
    "print(classification_report(\n",
    "    y_test, y_test_pred,\n",
    "    target_names=class_names,\n",
    "    digits=3\n",
    "))\n",
    "\n",
    "# Also compute macro and weighted F1 for easy comparison later\n",
    "macro_f1    = f1_score(y_test, y_test_pred, average='macro')\n",
    "weighted_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "print(f\"Macro F1:    {macro_f1:.4f}\")\n",
    "print(f\"Weighted F1: {weighted_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d2e3f4",
   "metadata": {},
   "source": [
    "### 1.7 — Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e3f4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Confusion Matrix Heatmap\n",
    "# =============================================================================\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "plt.figure(figsize=(9, 7))\n",
    "sns.heatmap(\n",
    "    cm, annot=True, fmt='d', cmap='Blues',\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    ")\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Node2Vec + Logistic Regression — Confusion Matrix (Test Set)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f4a5b6",
   "metadata": {},
   "source": [
    "### 1.8 — t-SNE Visualization of Node2Vec Embeddings\n",
    "\n",
    "t-SNE projects the 128-d embeddings down to 2D for visualization.  \n",
    "If Node2Vec has learned meaningful structure, we should see papers of the same class  \n",
    "forming clusters — even though the model **never saw the class labels or paper content**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a5b6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# t-SNE dimensionality reduction (128-d → 2-d)\n",
    "# =============================================================================\n",
    "print(\"Running t-SNE (this may take a moment)...\")\n",
    "\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=30,       # balance between local and global structure\n",
    "    learning_rate='auto',\n",
    "    n_iter=1000,\n",
    "    random_state=SEED,\n",
    "    init='pca',          # PCA initialization for stability\n",
    ")\n",
    "embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "print(f\"t-SNE output shape: {embeddings_2d.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b6c7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Plot t-SNE embeddings colored by class\n",
    "# =============================================================================\n",
    "# Use a colorblind-friendly palette\n",
    "palette = sns.color_palette('tab10', n_colors=len(class_names))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for idx, class_name in enumerate(class_names):\n",
    "    mask = labels == idx\n",
    "    plt.scatter(\n",
    "        embeddings_2d[mask, 0],\n",
    "        embeddings_2d[mask, 1],\n",
    "        c=[palette[idx]],\n",
    "        label=class_name,\n",
    "        alpha=0.6,\n",
    "        s=15,\n",
    "        edgecolors='none',\n",
    "    )\n",
    "\n",
    "plt.legend(title='Paper Class', bbox_to_anchor=(1.05, 1), loc='upper left', markerscale=3)\n",
    "plt.title('t-SNE of Node2Vec Embeddings (Cora — Structure Only)', fontsize=14)\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.grid(True, alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c7d8e9",
   "metadata": {},
   "source": [
    "### 1.9 — Summary & Key Takeaways\n",
    "\n",
    "**What we did:**\n",
    "- Trained Node2Vec on the Cora citation graph using **only the graph structure** (edges)\n",
    "- The model never saw the 1433-d BoW features or the class labels during embedding training\n",
    "- Used the learned 128-d embeddings as input to a simple Logistic Regression classifier\n",
    "<!-- \n",
    "**Key observations:**\n",
    "- Node2Vec achieves reasonable accuracy using citations alone, demonstrating  \n",
    "  that the citation network encodes meaningful information about paper topics\n",
    "- Papers in the same research area tend to cite each other, so structural proximity  \n",
    "  in the citation graph correlates with topical similarity\n",
    "- The t-SNE plot should show some class clustering, confirming the embeddings  \n",
    "  capture class-relevant structure -->\n",
    "\n",
    "**Limitations (motivating GNNs next):**\n",
    "- Node2Vec is **transductive**: it cannot generate embeddings for new/unseen nodes\n",
    "- It completely ignores the rich BoW features (paper content)\n",
    "- The embeddings are learned independently of the downstream task (unsupervised)\n",
    "\n",
    "**Next step:** Apply GCN and GAT (deep embeddings) that combine both graph structure  \n",
    "AND node features, with end-to-end supervised training for classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
